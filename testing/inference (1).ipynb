{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25cef450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.        , 0.        , ..., 0.00192261, 0.00299072,\n",
       "        0.00372314], dtype=float32),\n",
       " array([ 0.0039978 ,  0.00387573,  0.00332642, ...,  0.00030518,\n",
       "         0.00027466, -0.00088501], dtype=float32),\n",
       " array([-0.00167847, -0.00064087,  0.00088501, ...,  0.00024414,\n",
       "         0.00018311, -0.00018311], dtype=float32),\n",
       " array([-0.00054932, -0.00048828, -0.00018311, ..., -0.00079346,\n",
       "        -0.00082397, -0.00082397], dtype=float32),\n",
       " array([-0.00079346, -0.00082397, -0.00097656, ...,  0.00021362,\n",
       "         0.00021362,  0.00021362], dtype=float32),\n",
       " array([ 0.00021362,  0.00018311,  0.00018311, ..., -0.00024414,\n",
       "        -0.00018311, -0.00018311], dtype=float32),\n",
       " array([-0.00033569, -0.00057983, -0.0007019 , ...,  0.03363037,\n",
       "         0.03613281,  0.03805542], dtype=float32),\n",
       " array([0.03964233, 0.04019165, 0.03930664, ..., 0.02206421, 0.02246094,\n",
       "        0.0241394 ], dtype=float32),\n",
       " array([ 0.02816772,  0.03039551,  0.02612305, ...,  0.01550293,\n",
       "         0.00732422, -0.01483154], dtype=float32),\n",
       " array([-0.04244995, -0.05691528, -0.05117798, ..., -0.00332642,\n",
       "        -0.00393677, -0.00131226], dtype=float32),\n",
       " array([ 1.2512207e-03,  1.2817383e-03, -3.0517578e-05, ...,\n",
       "        -3.9672852e-04, -1.1901855e-03, -1.3427734e-03], dtype=float32),\n",
       " array([-0.00088501,  0.00030518,  0.0020752 , ..., -0.0067749 ,\n",
       "        -0.00564575, -0.00460815], dtype=float32),\n",
       " array([-0.00335693, -0.00189209, -0.00091553, ...,  0.0012207 ,\n",
       "         0.00061035, -0.00082397], dtype=float32),\n",
       " array([-0.00115967, -0.00018311,  0.00054932, ...,  0.0067749 ,\n",
       "         0.0078125 , -0.00186157], dtype=float32),\n",
       " array([-0.012146  , -0.0123291 , -0.0039978 , ..., -0.00061035,\n",
       "        -0.00057983, -0.00061035], dtype=float32),\n",
       " array([-7.0190430e-04, -7.9345703e-04, -7.6293945e-04, ...,\n",
       "         1.1596680e-03,  6.1035156e-04,  3.0517578e-05], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2965b325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Rate: 16000 Hz\n",
      "Total Frames: 16\n",
      "First Frame Shape: (160000,)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def split_audio_to_frames(audio_path, frame_duration, sampling_rate=None):\n",
    "    \"\"\"\n",
    "    Split an audio file into frames of specified duration.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        frame_duration (int): Frame duration in seconds.\n",
    "        sampling_rate (int, optional): Sampling rate for loading the audio. \n",
    "                                       If None, uses the original rate.\n",
    "    Returns:\n",
    "        list: List of NumPy arrays, each representing a frame.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    data, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "\n",
    "    # Convert frame duration to samples\n",
    "    frame_length = frame_duration * sr\n",
    "\n",
    "    # Split the audio into chunks\n",
    "    frames = [\n",
    "        data[i:i + frame_length] \n",
    "        for i in range(0, len(data), frame_length)\n",
    "    ]\n",
    "\n",
    "    return frames, sr\n",
    "\n",
    "# Parameters\n",
    "audio_path = \"test1.wav\"  # Replace with your audio file path\n",
    "frame_duration = 10       # Duration of each frame in seconds\n",
    "\n",
    "# Split the audio\n",
    "frames, sampling_rate = split_audio_to_frames(audio_path, frame_duration)\n",
    "\n",
    "# Print details\n",
    "print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "print(f\"Total Frames: {len(frames)}\")\n",
    "print(f\"First Frame Shape: {frames[0].shape}\")\n",
    "\n",
    "# Example: Save frames as individual arrays\n",
    "frame_arrays = [frame for frame in frames]\n",
    "\n",
    "# Optional: Save to a list\n",
    "audio_list = list(frame_arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823626f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ab9d7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutomaticSpeechRecognitionPipeline,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = \"/home/ec2-user/SageMaker/whisper_output_dir_10/checkpoint-520/\"\n",
    "# peft_model_id = \"openai/whisper-large-v3-turbo\"\n",
    "# peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n",
    "language = 'Arabic'\n",
    "task = 'transcribe'\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "pipe = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "\n",
    "def transcribe(audio):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        text = pipe(audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
    "    return text\n",
    "\n",
    "# transcribe(audio_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "925a350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التفرع بكل بشارع عبدالله غوشي\n",
      "التوحياته والله حالياً مسكر في دليفري بس من فرع فرع صوفيه في بلج طب غل لو انا بدي اجي اوبرع على ساعة شبه معي\n",
      "الصريح دي لا والله ما في غار دي بري بش انا بستلم عادي ما بصريح انداج بستلم ساعة سبعه من دي\n",
      " لا فما حد بتكثر الكريم بس والله ممنوع ولا حتى لو ما عطى الشريف لازم بس طيب ديليفر انا واقف عالباب بسمع عندك\n",
      "التوصيل\n",
      " احسسس\n",
      "التوسئئئئٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔ\n",
      "الجامعه الاجتونيه\n",
      "الجامعه\n",
      "التو\n",
      "السوه فيه فلج نفسها من جوا وين بشو فيه الزمق هلا تقريبا بالمنطقة الجنب سوبرماكز\n",
      "زاعتر ها د مكان موجود في مول اسمه صوفيه فيلج بالطابق الاردي موجود ام مقابل الشوير مول\n",
      "بس اي مجب نفس المنطقه يعني المنطقه نفس\n",
      "هو معروفه الكلمي اي بتسأل اي حد بصوفيه معروفه لصوفيه بلج هي زي مول مباني كلها مطاعم بتكون اي بلج\n",
      "فيجي وراه البرق الموجه هي اللي منطقه ثاني لا هي صوفيه بلجين بس ما ثواني\n",
      "تامم مايخدمه ثانيه افر افر افر افر افر افر افر افر افر افر افر افر افر افر افر اف افر اف اف اف\n"
     ]
    }
   ],
   "source": [
    "# 520\n",
    "for i in range(len(audio_list)):\n",
    "    print(transcribe(audio_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b999b2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التفضل\n",
      "التواته والله حالياً مسكر في ديليفري بس من فراع فراع صوفيه في بلج طب غال لو انا بدي اجي او برع ساعة ساعة مايس\n",
      "الصريح جزي لا والله ما في غير ديليفري بس انا بسلم عادي ما بسريح انداجي بسلم ساعة سبعه من دون ديليفري في مجاة\n",
      "لا فما حضرتك في الكريم بس والله ممنوع ولا حتى لو ما عطى الصريف لازم بس طيب ديليفير انا واقف على الباب بسمع عندك\n",
      "الفضل عالبه امر بس بدفع حدقتك يعني التوصيل فصوفي بلج يكون التوصيل\n",
      " ايه ايه ايه ايه ايه\n",
      "التوصيد لالاي مناطق عمان دينارين لا للسوفيه دينارين للمكان اللي المطعم فيه تمام لو لجمعه الاردنيه\n",
      "الجامعه الاردنيه\n",
      "والا عن جامعة الاميره السميه ولا نادي الخريجين شره ثوانيهين ثوانيهين ثوانيهين\n",
      "بصدوا عني فين هذا الطلاع على لي تلت دنامير طيب ماشي ويكون هس هس هس هو في وين\n",
      "السوفيه فيلج نفسها من جوا بين بشوثيه بالزرق هلا تقريبا بالمنطقة الجنب سبرماكز\n",
      "زاعتر هاد مكان موجود زي مول اسمه صوفيه فيلج بالطابق الارضي موجود ام مقابل الشوير مول\n",
      "بتزنيه الزعتار اللي هي بتيجي خلنا نتذكر وره البراكمولس اه بس انا بجي بنفس المنطقه يعني المنطقه نفس\n",
      "هو معروفه الكيميه اي بتسأل اي حد بصوفيه معروفه لصوفيه بلج هي زي مول مباني كلها مطاعم بتكون ايه بلج\n",
      "فيجي وراه البرك المول هي المنطقة ثاني لا هي صوفيه بلجين ما ثواني\n",
      "تامى مايخذ من ثانيه اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير اخير\n"
     ]
    }
   ],
   "source": [
    "# 500\n",
    "for i in range(len(audio_list)):\n",
    "    print(transcribe(audio_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0e3503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التفرع بكل بشارع عبدالله غوش مسكره\n",
      "والله حاليا مسكر في ديليفري بس من فرع فرع صفية في بلج طب غل لو انا بدي اجي او برع ساعة تبع مايس\n",
      "الصريح لا والله ما في غير دي بري بس انا بس لهم عادي ما بس ريح انداج بس لهم ساعة سبعة في مجال\n",
      "لا فامه حد بتكثر الكريم بس والله ممنوع ولا حتى لو ما عطه اصريف لازم بس طيب ديليفري انا واقف عالبق بس سمع انت\n",
      "التوصيل\n",
      " اشتركوا في القناة\n",
      "توصيل لأي مناطق عمان دينارين لا للصوفيه دينارين للمكان اللي المطعم فيه تمام لو لا الجامع الاردنيه\n",
      "الجامعه الارتونيه\n",
      "الجامعه\n",
      "النزل\n",
      "شوه فيه فلج نفسها من جوا بين بشوه فيه بالزبط هلا تقريبا بالمنطقة الجنب سبرماكز\n",
      "زاعتر هاد مكان موجود في مول اسمه صوفيه فيلج بالطابق الارضي موجود ام مقابل اي شوير مول\n",
      "بتزنيه\n",
      "هو معروفه الكلمي اي بتسال اي حد بصوفيه معروفه لصوفيه بلش هي زي مول مباني كلها مطاعم بتكون اي اه بل\n",
      "في جواره البرك الموضي المنطقة ثاني لا هي صوفيه بلجين ما ثواني جمب\n",
      " تمام مايخدم ثانيا؟ اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا اذا ا\n"
     ]
    }
   ],
   "source": [
    "# 370\n",
    "for i in range(len(audio_list)):\n",
    "    print(transcribe(audio_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb28bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الشبه المعك رأي تفضل عبدك العافيه الله يعافيك عالت فرع بكل بشار عبد الله غوشه مسكره\n",
      "والله حاليا مسكر في ديليفري بس من فرع فرع صوفيه في بلاج طب غا لو انا بدي اجي اوه برع ساعة ساعة مايس\n",
      "الصريح دي لا والله ما في غير دي لبري بس انا بستلم معادي ما بصريح ان دي بستلم ساعة سبعة من دون دي لبري في مجرى\n",
      "لا فما حد بتكثير الكريم بس والله ممنوع ولا حتى لو ما عطيه صريف لازم بس طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب طيب\n",
      "الفضل عالبه امر بس بدفع حدقتك يعني التوصيل في صوفيه بلج يكون التوصيل\n",
      " اشتركوا فيدينا\n",
      "توصيد لالاي مناطق عمان دينارين لا للصوفيه دينارين للمكان اللي المطعم فيه تمام لو لتجمع الاردنية\n",
      "الجامع الارتونيه\n",
      "والا\n",
      "بصدو عني فين هذا اللعلي تلت دنمير طيب ماشي ويكو هسه محاكم في طرع صوفي وين\n",
      "شوه فيه فلج نفسه من جوه وين بسوه في الزمق هلأ تقريبا بالمنطقة الجنب سوبرماركز\n",
      "زاعتر هاد مكان موجود زي مول اسمه صوفيه فيلج بالطابق الارضي موجود مقابل اي شوير مول\n",
      "بتزنيه الزعتار اللي بتيجي خلنا بكر ورا البركي مول صح بس انا مش بنفس المنطقة يعني المنطقة نفس\n",
      "هو معروف سيد الكيميه اي بتسأل اي حد بصوفيه معروفه لصوفيه فلش هي زي مول مباني كلها مطاعم بتكون اي اي\n",
      "في جواره والبرق الموال في المنطقه ثاني لا هي صوفيه بلجين ما ثواني جمبه\n",
      "تامم مايخدم ثانية؟ افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر افتر اف\n"
     ]
    }
   ],
   "source": [
    "# 300\n",
    "for i in range(len(audio_list)):\n",
    "    print(transcribe(audio_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9964bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التفضل\n",
      "التواته والله حالياً مسكر في ديليفري بس من فراعه فراعه صوفيه في بلج طب غا لو انا بدي اجي اوبره على الساعة\n",
      "الصريح جد لا والله ما في غار ديليفري بش انا بسلم عادي ما بسرح انداجي بسلم ساعة سبعه من ديليفري في مجايا\n",
      "الكريم لا فا ما حد بتكسر الكريم بس والله ممنوع ولا حتى لو ما عطى الصريف لازم بس طيب ديليفري انا واقف على الباب بس سمع\n",
      "تدفع على البحه امر بس بدفع حدقتك يعني التوصيل فصفي بلج يكون التوصيل\n",
      " لس في ديناريه\n",
      "التوصيد لايمناطق عمان دينارين لا للسوفيه دينارين للمكان اللي المطعم فيه تمام لو للجامعه الاردنيه\n",
      "الجامعه الاردنيه\n",
      "ولا عن جامعة الاميره السميه ولا نادي الخريجين شرر ثوانيهين ثوانيهين ثوانيهين ثوانيهين\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بصدو عني فين هذا الطلاعه للي تلت دنمير طيب و هس هس\n",
      "الصوصيه فلج نفسها من جوا وين بشوصيه بالزرق هلا تقريبا بالمنطقه الجنب سوبرماكز\n",
      "زاعتر هده مكان موجود في مول اسمه صوفيه فيلج بالطابق الارضي موجود ام مقابل\n",
      "بتزنيه الزعتار اللي هي بتيجي خلنا نذكر ورا البركيمولف اه بس انا بجب نفس المنطقه يعني المنطقه نفس\n",
      "هو معروفه الكيمئي اي بتسال اي حد بصوفئه معروفه لصوفئه هي زي مول مباني كلها مطاعم بتكون ايه بلج\n",
      "فيجي وراه البرق المول هي اللي منطقه ثاني لا هي صوفيه بلجين ما ثواني\n",
      "تاما ما يخذ ما ثانيه افر افر افر افر افر افكر بذل خبره اعطيك\n"
     ]
    }
   ],
   "source": [
    "# 1000\n",
    "for i in range(len(audio_list)):\n",
    "    print(transcribe(audio_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40f055b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التفرع بكل بشارع عبدالله غوشي\n",
      "التوحياته والله حالياً مسكر في دليفري بس من فرع فرع صوفيه في بلج طب غل لو انا بدي اجي اوبرع على ساعة شبه معي\n",
      "الصريح دي لا والله ما في غار دي بري بش انا بستلم عادي ما بصريح انداج بستلم ساعة سبعه من دي\n",
      " لا فما حد بتكثر الكريم بس والله ممنوع ولا حتى لو ما عطى الشريف لازم بس طيب ديليفر انا واقف عالباب بسمع عندك\n",
      "التوصيل\n",
      " احسسس\n",
      "التوسئئئئٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔٔ\n",
      "الجامعه الاجتونيه\n",
      "الجامعه\n",
      "التو\n",
      "السوه فيه فلج نفسها من جوا وين بشو فيه الزمق هلا تقريبا بالمنطقة الجنب سوبرماكز\n",
      "زاعتر ها د مكان موجود في مول اسمه صوفيه فيلج بالطابق الاردي موجود ام مقابل الشوير مول\n",
      "بس اي مجب نفس المنطقه يعني المنطقه نفس\n",
      "هو معروفه الكلمي اي بتسأل اي حد بصوفيه معروفه لصوفيه بلج هي زي مول مباني كلها مطاعم بتكون اي بلج\n",
      "فيجي وراه البرق الموجه هي اللي منطقه ثاني لا هي صوفيه بلجين بس ما ثواني\n",
      "تامم مايخدمه ثانيه افر افر افر افر افر افر افر افر افر افر افر افر افر افر افر اف افر اف اف اف\n"
     ]
    }
   ],
   "source": [
    "#520\n",
    "for i in range(len(audio_list)):\n",
    "    print(transcribe(audio_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "068adb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Rate: 16000 Hz\n",
      "Audio Array: [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.15966797e-03\n",
      " 6.10351562e-04 3.05175781e-05]\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Load audio file\n",
    "data, sampling_rate = sf.read(\"test1.wav\")\n",
    "\n",
    "# Print details\n",
    "print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "print(f\"Audio Array: {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced9f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is valid and can be read.\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    data, samplerate = sf.read(\"test1.wav\")\n",
    "    print(\"File is valid and can be read.\")\n",
    "except Exception as e:\n",
    "    print(f\"Audio file issue: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ddc124b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c2acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "audio, sr = librosa.load(\"test.wav\", sr=16000)  # Resample to 16kHz if needed\n",
    "sf.write(\"test1.wav\", audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcaa3da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading: Error opening 's3://stt-jordan-dataset/WhatsApp Audio 2024-12-10 at 20.31.24_04c1297c.wav': System error.\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    audio_data, samplerate = sf.read(\"s3://stt-jordan-dataset/WhatsApp Audio 2024-12-10 at 20.31.24_04c1297c.wav\")\n",
    "    print(\"File is valid.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c8392e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg: error while loading shared libraries: libopenh264.so.5: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Soundfile is either not in the correct format or is malformed. Ensure that the soundfile has a valid audio file extension (e.g. wav, flac or mp3) and is not corrupted. If reading from a remote URL, ensure that the URL is the full address to **download** the audio file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     result \u001b[38;5;241m=\u001b[39m pipe(audio)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m \u001b[43mofficial_transcribe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest1.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mofficial_transcribe\u001b[0;34m(audio)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mofficial_transcribe\u001b[39m(audio):\n\u001b[0;32m---> 29\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:283\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    224\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py:1294\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:364\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[0;34m(self, inputs, chunk_length_s, stride_length_s)\u001b[0m\n\u001b[1;32m    361\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m--> 364\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    367\u001b[0m extra \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/audio_utils.py:41\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[0;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[1;32m     39\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(out_bytes, np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoundfile is either not in the correct format or is malformed. Ensure that the soundfile has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma valid audio file extension (e.g. wav, flac or mp3) and is not corrupted. If reading from a remote \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL, ensure that the URL is the full address to **download** the audio file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio\n",
      "\u001b[0;31mValueError\u001b[0m: Soundfile is either not in the correct format or is malformed. Ensure that the soundfile has a valid audio file extension (e.g. wav, flac or mp3) and is not corrupted. If reading from a remote URL, ensure that the URL is the full address to **download** the audio file."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, language='Arabic',task='transcribe')\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "def official_transcribe(audio):\n",
    "    result = pipe(audio)\n",
    "    return result[\"text\"]\n",
    "\n",
    "official_transcribe(\"test1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c3ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
